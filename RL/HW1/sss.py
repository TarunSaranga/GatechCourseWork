"A discounted MDP solved using the value iteration algorithm.\n\n    Description\n    -----------\n    ValueIteration applies the value iteration algorithm to solve a\n    discounted MDP. The algorithm consists of solving Bellman's equation\n    iteratively.\n    Iteration is stopped when an epsilon-optimal policy is found or after a\n    specified number (``max_iter``) of iterations.\n    This function uses verbose and silent modes. In verbose mode, the function\n    displays the variation of ``V`` (the value function) for each iteration and\n    the condition which stopped the iteration: epsilon-policy found or maximum\n    number of iterations reached.\n\n    Parameters\n    ----------\n    transitions : array\n        Transition probability matrices. See the documentation for the ``MDP``\n        class for details.\n    reward : array\n        Reward matrices or vectors. See the documentation for the ``MDP`` class\n        for details.\n    discount : float\n        Discount factor. See the documentation for the ``MDP`` class for\n        details.\n    epsilon : float, optional\n        Stopping criterion. See the documentation for the ``MDP`` class for\n        details.  Default: 0.01.\n    max_iter : int, optional\n        Maximum number of iterations. If the value given is greater than a\n        computed bound, a warning informs that the computed bound will be used\n        instead. By default, if ``discount`` is not equal to 1, a bound for\n        ``max_iter`` is computed, otherwise ``max_iter`` = 1000. See the\n        documentation for the ``MDP`` class for further details.\n    initial_value : array, optional\n        The starting value function. Default: a vector of zeros.\n\n    Data Attributes\n    ---------------\n    V : tuple\n        The optimal value function.\n    policy : tuple\n        The optimal policy function. Each element is an integer corresponding\n        to an action which maximises the value function in that state.\n    iter : int\n        The number of iterations taken to complete the computation.\n    time : float\n        The amount of CPU time used to run the algorithm.\n\n    Methods\n    -------\n    run()\n        Do the algorithm iteration.\n    setSilent()\n        Sets the instance to silent mode.\n    setVerbose()\n        Sets the instance to verbose mode.\n\n    Notes\n    -----\n    In verbose mode, at each iteration, displays the variation of V\n    and the condition which stopped iterations: epsilon-optimum policy found\n    or maximum number of iterations reached.\n\n    Examples\n    --------\n    >>> import mdptoolbox, mdptoolbox.example\n    >>> P, R = mdptoolbox.example.forest()\n    >>> vi = mdptoolbox.mdp.ValueIteration(P, R, 0.96)\n    >>> vi.verbose\n    False\n    >>> vi.run()\n    >>> expected = (5.93215488, 9.38815488, 13.38815488)\n    >>> all(expected[k] - vi.V[k] < 1e-12 for k in range(len(expected)))\n    True\n    >>> vi.policy\n    (0, 0, 0)\n    >>> vi.iter\n    4\n\n    >>> import mdptoolbox\n    >>> import numpy as np\n    >>> P = np.array([[[0.5, 0.5],[0.8, 0.2]],[[0, 1],[0.1, 0.9]]])\n    >>> R = np.array([[5, 10], [-1, 2]])\n    >>> vi = mdptoolbox.mdp.ValueIteration(P, R, 0.9)\n    >>> vi.run()\n    >>> expected = (40.048625392716815, 33.65371175967546)\n    >>> all(expected[k] - vi.V[k] < 1e-12 for k in range(len(expected)))\n    True\n    >>> vi.policy\n    (1, 0)\n    >>> vi.iter\n    26\n\n    >>> import mdptoolbox\n    >>> import numpy as np\n    >>> from scipy.sparse import csr_matrix as sparse\n    >>> P = [None] * 2\n    >>> P[0] = sparse([[0.5, 0.5],[0.8, 0.2]])\n    >>> P[1] = sparse([[0, 1],[0.1, 0.9]])\n    >>> R = np.array([[5, 10], [-1, 2]])\n    >>> vi = mdptoolbox.mdp.ValueIteration(P, R, 0.9)\n    >>> vi.run()\n    >>> expected = (40.048625392716815, 33.65371175967546)\n    >>> all(expected[k] - vi.V[k] < 1e-12 for k in range(len(expected)))\n    True\n    >>> vi.policy\n    (1, 0)\n\n    "
"A discounted MDP solved using the value iteration algorithm.\n\n    Description\n    -----------\n    ValueIteration applies the value iteration algorithm to solve a\n    discounted MDP. The algorithm consists of solving Bellman's equation\n    iteratively.\n    Iteration is stopped when an epsilon-optimal policy is found or after a\n    specified number (``max_iter``) of iterations.\n    This function uses verbose and silent modes. In verbose mode, the function\n    displays the variation of ``V`` (the value function) for each iteration and\n    the condition which stopped the iteration: epsilon-policy found or maximum\n    number of iterations reached.\n\n    Parameters\n    ----------\n    transitions : array\n        Transition probability matrices. See the documentation for the ``MDP``\n        class for details.\n    reward : array\n        Reward matrices or vectors. See the documentation for the ``MDP`` class\n        for details.\n    discount : float\n        Discount factor. See the documentation for the ``MDP`` class for\n        details.\n    epsilon : float, optional\n        Stopping criterion. See the documentation for the ``MDP`` class for\n        details.  Default: 0.01.\n    max_iter : int, optional\n        Maximum number of iterations. If the value given is greater than a\n        computed bound, a warning informs that the computed bound will be used\n        instead. By default, if ``discount`` is not equal to 1, a bound for\n        ``max_iter`` is computed, otherwise ``max_iter`` = 1000. See the\n        documentation for the ``MDP`` class for further details.\n    initial_value : array, optional\n        The starting value function. Default: a vector of zeros.\n\n    Data Attributes\n    ---------------\n    V : tuple\n        The optimal value function.\n    policy : tuple\n        The optimal policy function. Each element is an integer corresponding\n        to an action which maximises the value function in that state.\n    iter : int\n        The number of iterations taken to complete the computation.\n    time : float\n        The amount of CPU time used to run the algorithm.\n\n    Methods\n    -------\n    run()\n        Do the algorithm iteration.\n    setSilent()\n        Sets the instance to silent mode.\n    setVerbose()\n        Sets the instance to verbose mode.\n\n    Notes\n    -----\n    In verbose mode, at each iteration, displays the variation of V\n    and the condition which stopped iterations: epsilon-optimum policy found\n    or maximum number of iterations reached.\n\n    Examples\n    --------\n    >>> import mdptoolbox, mdptoolbox.example\n    >>> P, R = mdptoolbox.example.forest()\n    >>> vi = mdptoolbox.mdp.ValueIteration(P, R, 0.96)\n    >>> vi.verbose\n    False\n    >>> vi.run()\n    >>> expected = (5.93215488, 9.38815488, 13.38815488)\n    >>> all(expected[k] - vi.V[k] < 1e-12 for k in range(len(expected)))\n    True\n    >>> vi.policy\n    (0, 0, 0)\n    >>> vi.iter\n    4\n\n    >>> import mdptoolbox\n    >>> import numpy as np\n    >>> P = np.array([[[0.5, 0.5],[0.8, 0.2]],[[0, 1],[0.1, 0.9]]])\n    >>> R = np.array([[5, 10], [-1, 2]])\n    >>> vi = mdptoolbox.mdp.ValueIteration(P, R, 0.9)\n    >>> vi.run()\n    >>> expected = (40.048625392716815, 33.65371175967546)\n    >>> all(expected[k] - vi.V[k] < 1e-12 for k in range(len(expected)))\n    True\n    >>> vi.policy\n    (1, 0)\n    >>> vi.iter\n    26\n\n    >>> import mdptoolbox\n    >>> import numpy as np\n    >>> from scipy.sparse import csr_matrix as sparse\n    >>> P = [None] * 2\n    >>> P[0] = sparse([[0.5, 0.5],[0.8, 0.2]])\n    >>> P[1] = sparse([[0, 1],[0.1, 0.9]])\n    >>> R = np.array([[5, 10], [-1, 2]])\n    >>> vi = mdptoolbox.mdp.ValueIteration(P, R, 0.9)\n    >>> vi.run()\n    >>> expected = (40.048625392716815, 33.65371175967546)\n    >>> all(expected[k] - vi.V[k] < 1e-12 for k in range(len(expected)))\n    True\n    >>> vi.policy\n    (1, 0)\n\n    "
print("A discounted MDP solved using the value iteration algorithm.\n\n   Description\n    -----------\n    ValueIteration applies the value iteration algorithm to solve a\n    discounted MDP. The algorithm consists of solving Bellman's equation\n    iteratively.\n    Iteration is stopped when an epsilon-optimal policy is found or after a\n    specified number (``max_iter``) of iterations.\n    This function uses verbose and silent modes. In verbose mode, the function\n    displays the variation of ``V`` (the value function) for each iteration and\n    the condition which stopped the iteration: epsilon-policy found or maximum\n    number of iterations reached.\n\n    Parameters\n    ----------\n    transitions : array\n        Transition probability matrices. See the documentation for the ``MDP``\n        class for details.\n    reward : array\n        Reward matrices or vectors. See the documentation for the ``MDP`` class\n        for details.\n    discount : float\n        Discount factor. See the documentation for the ``MDP`` class for\n        details.\n    epsilon : float, optional\n        Stopping criterion. See the documentation for the ``MDP`` class for\n        details.  Default: 0.01.\n    max_iter : int, optional\n        Maximum number of iterations. If the value given is greater than a\n        computed bound, a warning informs that the computed bound will be used\n        instead. By default, if ``discount`` is not equal to 1, a bound for\n        ``max_iter`` is computed, otherwise ``max_iter`` = 1000. See the\n        documentation for the ``MDP`` class for further details.\n    initial_value : array, optional\n        The starting value function. Default: a vector of zeros.\n\n    Data Attributes\n    ---------------\n    V : tuple\n        The optimal value function.\n    policy : tuple\n        The optimal policy function. Each element is an integer corresponding\n        to an action which maximises the value function in that state.\n    iter : int\n        The number of iterations taken to complete the computation.\n    time : float\n        The amount of CPU time used to run the algorithm.\n\n    Methods\n    -------\n    run()\n        Do the algorithm iteration.\n    setSilent()\n        Sets the instance to silent mode.\n    setVerbose()\n        Sets the instance to verbose mode.\n\n    Notes\n    -----\n    In verbose mode, at each iteration, displays the variation of V\n    and the condition which stopped iterations: epsilon-optimum policy found\n    or maximum number of iterations reached.\n\n    Examples\n    --------\n    >>> import mdptoolbox, mdptoolbox.example\n    >>> P, R = mdptoolbox.example.forest()\n    >>> vi = mdptoolbox.mdp.ValueIteration(P, R, 0.96)\n    >>> vi.verbose\n    False\n    >>> vi.run()\n    >>> expected = (5.93215488, 9.38815488, 13.38815488)\n    >>> all(expected[k] - vi.V[k] < 1e-12 for k in range(len(expected)))\n    True\n    >>> vi.policy\n    (0, 0, 0)\n    >>> vi.iter\n    4\n\n    >>> import mdptoolbox\n    >>> import numpy as np\n    >>> P = np.array([[[0.5, 0.5],[0.8, 0.2]],[[0, 1],[0.1, 0.9]]])\n    >>> R = np.array([[5, 10], [-1, 2]])\n    >>> vi = mdptoolbox.mdp.ValueIteration(P, R, 0.9)\n    >>> vi.run()\n    >>> expected = (40.048625392716815, 33.65371175967546)\n    >>> all(expected[k] - vi.V[k] < 1e-12 for k in range(len(expected)))\n    True\n    >>> vi.policy\n    (1, 0)\n    >>> vi.iter\n    26\n\n    >>> import mdptoolbox\n    >>> import numpy as np\n    >>> from scipy.sparse import csr_matrix as sparse\n    >>> P = [None] * 2\n    >>> P[0] = sparse([[0.5, 0.5],[0.8, 0.2]])\n    >>> P[1] = sparse([[0, 1],[0.1, 0.9]])\n    >>> R = np.array([[5, 10], [-1, 2]])\n    >>> vi = mdptoolbox.mdp.ValueIteration(P, R, 0.9)\n    >>> vi.run()\n    >>> expected = (40.048625392716815, 33.65371175967546)\n    >>> all(expected[k] - vi.V[k] < 1e-12 for k in range(len(expected)))\n    True\n    >>> vi.policy\n    (1, 0)\n\n    ")
